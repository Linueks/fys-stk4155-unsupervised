<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html Clustering.do.txt 
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title></title>
<style type="text/css">
/* blueish style */
body {
  margin-top: 1.0em;
  background-color: #ffffff;
  font-family: Helvetica, Arial, FreeSans, san-serif;
  color: #000000;
}
h1 { font-size: 1.8em; color: #1e36ce; }
h2 { font-size: 1.6em; color: #1e36ce; }
h3 { font-size: 1.4em; color: #1e36ce; }
h4 { font-size: 1.2em; color: #1e36ce; }
a { color: #1e36ce; text-decoration:none; }
tt { font-family: "Courier New", Courier; }
p { text-indent: 0px; }
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-style: normal; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa; }
/* pre style removed because it will interfer with pygments */
div.highlight {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    line-height: 1.21429em;
}
div.cell {
    width: 100%;
    padding: 5px 5px 5px 0;
    margin: 0;
    outline: none;
}
div.input {
    page-break-inside: avoid;
    box-orient: horizontal;
    box-align: stretch;
    display: flex;
    flex-direction: row;
    align-items: stretch;
}
div.inner_cell {
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
    box-flex: 1;
    flex: 1;
}
div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 4px;
    background: #f7f7f7;
    line-height: 1.21429em;
}
div.input_area > div.highlight {
    margin: .4em;
    border: none;
    padding: 0;
    background-color: transparent;
}
div.output_wrapper {
    position: relative;
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
}
.output {
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
}
div.output_area {
    padding: 0;
    page-break-inside: avoid;
    box-orient: horizontal;
    box-align: stretch;
    display: flex;
    flex-direction: row;
    align-items: stretch;
}
div.output_subarea {
    padding: .4em .4em 0 .4em;
    box-flex: 1;
    flex: 1;
}
div.output_text {
    text-align: left;
    color: #000;
    line-height: 1.21429em;
}
div { text-align: justify; text-justify: inter-word; }
.tab {
  padding-left: 1.5em;
}
div.toc p,a {
  line-height: 1.3;
  margin-top: 1.1;
  margin-bottom: 1.1;
}
</style>
</head>

<!-- tocinfo
{'highest level': 1,
 'sections': [('Clustering Analysis', 1, None, 'clustering-analysis'),
              ('Basic Idea of the K-means Clustering Algorithm',
               2,
               None,
               'basic-idea-of-the-k-means-clustering-algorithm'),
              ('The K-means Clustering Algorithm',
               2,
               None,
               'the-k-means-clustering-algorithm'),
              ('Writing Our Own Code', 2, None, 'writing-our-own-code'),
              ('Basic Python', 3, None, 'basic-python'),
              ('Towards a More Numpythonic Code',
               3,
               None,
               'towards-a-more-numpythonic-code'),
              ('Onto Bigger and Better Things',
               3,
               None,
               'onto-bigger-and-better-things'),
              ('Benchmarking and Testing Our Code',
               2,
               None,
               'benchmarking-and-testing-our-code'),
              ('Vector Quantization: Actual Example Use Case',
               2,
               None,
               'vector-quantization-actual-example-use-case')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- ------------------- main content ---------------------- -->
<h1 id="clustering-analysis">Clustering Analysis </h1>
<p>In this chapter we will concern ourselves with the study of <b>cluster analysis</b>.
In general terms cluster analysis, or clustering, is the task of grouping a
data-set into different distinct categories based on some measure of equality of
the data. This measure is often referred to as a <b>metric</b> or <b>similarity
measure</b> in the literature (note: sometimes we deal with a <b>dissimilarity
measure</b> instead). Usually, these metrics are formulated as some kind of
distance function between points in a high-dimensional space.
</p>

<p>There exists a lot of such distance measures. The simplest, and also the most
common is the <b>Euclidean distance</b> (i.e. Pythagoras). A good source for those of
you wanting a thorough overview is the article (DOI:10.5120/ijca2016907841
Irani, Pise, Phatak). A few other metrics mentioned there are: <em>cosine
similarity</em>, <em>Manhattan distance</em>, <em>Chebychev distance</em> and the <em>Minkowski
distance</em>. The Minkowski distance is a general formulation which encapsulates a
range of metrics. All of these, and many more, can be used in clustering. There
exists different categories of clustering algorithms. A few of the most
common are: <em>centroid-</em>, <em>distribution-</em>, <em>density-</em> and <em>hierarchical-
clustering</em>. We will concern ourselves primarily with the first one.
</p>
<h2 id="basic-idea-of-the-k-means-clustering-algorithm">Basic Idea of the K-means Clustering Algorithm </h2>
<p>The simplest of all clustering algorithms is the aptly named <b>k-means algorithm</b>
, sometimes also referred to as <em>Lloyds algorithm</em>. It is the simplest and also
the most common. From its simplicity it obtains both strengths and weaknesses.
These will be discussed in more detail later. The k-means algorithm is a
<b>centroid based</b> clustering algorithm.
</p>

<p>Assume, we are given \( n \) data points and we wish to split the data into \( K < n \)
different categories, or clusters. We label each cluster by an integer \( k\in\{
1, \cdots, K \} \). In the basic k-means algorithm each point is assigned to only
one cluster \( k \), and these assignments are <em>non-injective</em> i.e. many-to-one. We
can think of these mappings as an encoder \( k = C(i) \), which assigns the \( i \)-th
data-point \( \bf x_i \) to the \( k \)-th cluster. Before we jump into the mathematics
let us describe the k-means algorithm in words:
</p>
<ol>
<li> We start with guesses / random initializations of our \( k \) cluster centers / centroids</li>
<li> For each centroid the points that are most similar are identified</li>
<li> Then we move / replace each centroid with a coordinate average of all the points that were assigned to that centroid.</li>
<li> Iterate this points 2, 3) until the centroids no longer move (to some tolerance)</li>
</ol>
<p>Now we consider the method formally. Again, we assume we have \( n \) data-points
(vectors)
</p>
$$
\begin{equation}\label{eq:kmeanspoints}
  \boldsymbol{x_i}  = \{x_{i, 1}, \cdots, x_{i, p}\}\in\mathbb{R}^p.
\end{equation}
$$

<p>which we wish to group into \( K < n \) clusters. For our dissimilarity measure we
will use the <em>squared Euclidean distance</em>
</p>
$$
\begin{equation}\label{eq:squaredeuclidean}
  d(\boldsymbol{x_i}, \boldsymbol{x_i'}) = \sum_{j=1}^p(x_{ij} - x_{i'j})^2
                         = ||\boldsymbol{x_i} - \boldsymbol{x_{i'}}||^2
\end{equation}
$$

<p>Next we define the so called <em>within-cluster point scatter</em> which gives us a
measure of how close each data point assigned to the same cluster tends to be to
the all the others.
</p>
$$
\begin{equation}\label{eq:withincluster}
  W(C) = \frac{1}{2}\sum_{k=1}^K\sum_{C(i)=k}
          \sum_{C(i')=k}d(\boldsymbol{x_i}, \boldsymbol{x_{i'}}) =
          \sum_{k=1}^KN_k\sum_{C(i)=k}||\boldsymbol{x_i} - \boldsymbol{\overline{x_k}}||^2
\end{equation}
$$

<p>where \( \boldsymbol{\overline{x_k}} \) is the mean vector associated with the \( k \)-th
cluster, and \( N_k = \sum_{i=1}^nI(C(i) = k) \), where the \( I() \) notation is
similar to the Kronecker delta (<em>Commonly used in statistics, it just means that
when \( i = k \) we have the encoder \( C(i) \)</em>). In other words,  the within-cluster
scatter measures the compactness of each cluster with respect to the data points
assigned to each cluster. This is the quantity that the \( k \)-means algorithm aims
to minimize. We refer to this quantity \( W(C) \) as the within cluster scatter
because of its relation to the <em>total scatter</em>.
</p>
$$
\begin{equation}\label{eq:totalscatter}
  T = W(C) + B(C) = \frac{1}{2}\sum_{i=1}^n
                    \sum_{i'=1}^nd(\boldsymbol{x_i}, \boldsymbol{x_{i'}})
                  = \frac{1}{2}\sum_{k=1}^K\sum_{C(i)=k}
                    \Big(\sum_{C(i') = k}d(\boldsymbol{x_i}, \boldsymbol{x_{i'}})
                  + \sum_{C(i')\neq k}d(\boldsymbol{x_i}, \boldsymbol{x_{i'}})\Big)
\end{equation}
$$

<p>Which is a quantity that is conserved throughout the \( k \)-means algorithm. It can
be thought of as the total amount of information in the data, and it is composed
of the aforementioned within-cluster scatter and the <em>between-cluster scatter</em>
\( B(C) \). In methods such as principle component analysis the total scatter is not
conserved.
</p>

<p>Given a cluster mean \( \boldsymbol{m_k} \) we define the <b>total cluster variance</b></p>
$$
\begin{equation}\label{eq:totalclustervariance}
  \min_{C, \{\boldsymbol{m_k}\}_1^K}\sum_{k=1}^KN_k\sum||\boldsymbol{x_i} - \boldsymbol{m_k}||^2
\end{equation}
$$

<p>Now we have all the pieces necessary to formally revisit the k-means algorithm.
If you at this point feel like some of the above definitions came a bit out of
no-where, don't fret, the method does get a whole lot simpler once we start
programming.
</p>
<h2 id="the-k-means-clustering-algorithm">The K-means Clustering Algorithm </h2>
<p>The k-means clustering algorithm goes as follows (note in my opinion this
description is a bit complicated and is lifted directly out of ESL HASTIE for
deeper understanding purposes)
</p>

<ol>
<li> For a given cluster assignment \( C \), and \( k \) cluster means \( \{m_1, \cdots, m_k\} \). We minimize the total cluster variance with respect to the cluster means \( \{m_k\} \) yielding the means of the currently assigned clusters.</li>
<li> Given a current set of \( k \) means \( \{m_k\} \) the total cluster variance is minimized by assigning each observation to the closest (current) cluster mean. That is $$C(i) = \underset{1\leq k\leq K}{\mathrm{argmin}} ||\boldsymbol{x_i} - \boldsymbol{m_k}||^2$$</li>
<li> Steps 1 and 2 are repeated until the assignments do not change.</li>
</ol>
<p>As previously stated the above formulation can be a bit difficult to understand,
<em>at least the first time</em>, due to the dense notation used. But all in all the
concept is fairly simple when explained in words. The math needs to be
understood but to help you along the way we summarize the algorithm as follows
(try to look at the terms above to match with the summary).
</p>

<ol>
<li> Before we start we specify a number \( k \) which is the number of clusters we want to try to separate our data into.</li>
<li> We initially choose \( k \) random data points in our data as our initial centroids, <em>or means</em> (this is where the name comes from).</li>
<li> Assign each data point to their closest centroid, based on the squared Euclidean distance.</li>
<li> For each of the \( k \) cluster we update the centroid by calculating new mean values for all the data points in the cluster.</li>
<li> Iteratively minimize the within cluster scatter by performing steps (3, 4) until the new assignments stop changing (can be to some tolerance) or until a maximum number of iterations have passed.</li>
</ol>
<p>That's it, nothing magical happening.</p>
<h2 id="writing-our-own-code">Writing Our Own Code </h2>
<p>In the following section we will work to develop a deeper understanding of the
previously discussed mathematics through developing codes to do k-means cluster
analysis.
</p>
<h3 id="basic-python">Basic Python </h3>
<p>Let us now program the most basic version of the algorithm using nothing but
Python with numpy arrays. This code is kept intentionally simple to gradually
progress our understanding. There is no vectorization of any kind, and even most
helper functions are not utilized. Throughout our implementation process it will
be helpful to keep in mind both the mathematical description of the algorithm
<em>and</em> our summary from above. In addition, try to think of ways to optimize this
while reading the next section. We will get to it, take it as a challenge to see
if your optimizations are better.
</p>

<p>First of all we need a dataset to do our cluster analysis on, for clarity (and
lack of googling beforehand) we generate it ourselves using Gaussians. First we
import
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">time</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">tensorflow</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">tf</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">matplotlib</span> <span style="color: #008000; font-weight: bold">import</span> image
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.cluster</span> <span style="color: #008000; font-weight: bold">import</span> KMeans
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">IPython.display</span> <span style="color: #008000; font-weight: bold">import</span> display

np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">2021</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Next we define functions, for ease of use later, to generate Gaussians and to
set up our toy data set.
</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">gaussian_points</span>(dim<span style="color: #666666">=2</span>, n_points<span style="color: #666666">=1000</span>, mean_vector<span style="color: #666666">=</span>np<span style="color: #666666">.</span>array([<span style="color: #666666">0</span>, <span style="color: #666666">0</span>]),
                    sample_variance<span style="color: #666666">=1</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Very simple custom function to generate gaussian distributed point clusters</span>
<span style="color: #BA2121; font-style: italic">    with variable dimension, number of points, means in each direction</span>
<span style="color: #BA2121; font-style: italic">    (must match dim) and sample variance.</span>

<span style="color: #BA2121; font-style: italic">    Inputs:</span>
<span style="color: #BA2121; font-style: italic">        dim (int)</span>
<span style="color: #BA2121; font-style: italic">        n_points (int)</span>
<span style="color: #BA2121; font-style: italic">        mean_vector (np.array) (where index 0 is x, index 1 is y etc.)</span>
<span style="color: #BA2121; font-style: italic">        sample_variance (float)</span>

<span style="color: #BA2121; font-style: italic">    Returns:</span>
<span style="color: #BA2121; font-style: italic">        data (np.array): with dimensions (dim x n_points)</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>

    mean_matrix <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(dim) <span style="color: #666666">+</span> mean_vector
    covariance_matrix <span style="color: #666666">=</span> np<span style="color: #666666">.</span>eye(dim) <span style="color: #666666">*</span> sample_variance
    data <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>multivariate_normal(mean_matrix, covariance_matrix,
                                    n_points)
    <span style="color: #008000; font-weight: bold">return</span> data



<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">generate_simple_clustering_dataset</span>(dim<span style="color: #666666">=2</span>, n_points<span style="color: #666666">=1000</span>, plotting<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,
                                    return_data<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Toy model to illustrate k-means clustering</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>

    data1 <span style="color: #666666">=</span> gaussian_points(mean_vector<span style="color: #666666">=</span>np<span style="color: #666666">.</span>array([<span style="color: #666666">5</span>, <span style="color: #666666">5</span>]))
    data2 <span style="color: #666666">=</span> gaussian_points()
    data3 <span style="color: #666666">=</span> gaussian_points(mean_vector<span style="color: #666666">=</span>np<span style="color: #666666">.</span>array([<span style="color: #666666">1</span>, <span style="color: #666666">4.5</span>]))
    data4 <span style="color: #666666">=</span> gaussian_points(mean_vector<span style="color: #666666">=</span>np<span style="color: #666666">.</span>array([<span style="color: #666666">5</span>, <span style="color: #666666">1</span>]))
    data <span style="color: #666666">=</span> np<span style="color: #666666">.</span>concatenate((data1, data2, data3, data4), axis<span style="color: #666666">=0</span>)

    <span style="color: #008000; font-weight: bold">if</span> plotting:
        fig, ax <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplots()
        ax<span style="color: #666666">.</span>scatter(data[:, <span style="color: #666666">0</span>], data[:, <span style="color: #666666">1</span>], alpha<span style="color: #666666">=0.2</span>)
        ax<span style="color: #666666">.</span>set_title(<span style="color: #BA2121">&#39;Toy Model Dataset&#39;</span>)
        plt<span style="color: #666666">.</span>show()


    <span style="color: #008000; font-weight: bold">if</span> return_data:
        <span style="color: #008000; font-weight: bold">return</span> data


data <span style="color: #666666">=</span> generate_simple_clustering_dataset()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Now that we are our, albeit very simple, dataset we are ready to start
implementing the k-means algorithm.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">n_samples, dimensions <span style="color: #666666">=</span> data<span style="color: #666666">.</span>shape
n_clusters <span style="color: #666666">=</span> <span style="color: #666666">4</span>

<span style="color: #408080; font-style: italic"># we randomly initialize our centroids</span>
np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">2021</span>)
centroids <span style="color: #666666">=</span> data[np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>choice(n_samples, n_clusters, replace<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>), :]
distances <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((n_samples, n_clusters))

<span style="color: #408080; font-style: italic"># first we need to calculate the distance to each centroid from our data</span>
<span style="color: #008000; font-weight: bold">for</span> k <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_clusters):
    <span style="color: #008000; font-weight: bold">for</span> n <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_samples):
        dist <span style="color: #666666">=</span> <span style="color: #666666">0</span>
        <span style="color: #008000; font-weight: bold">for</span> d <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(dimensions):
            dist <span style="color: #666666">+=</span> np<span style="color: #666666">.</span>abs(data[n, d] <span style="color: #666666">-</span> centroids[k, d])<span style="color: #666666">**2</span>
            distances[n, k] <span style="color: #666666">=</span> dist

<span style="color: #408080; font-style: italic"># we initialize an array to keep track of to which cluster each point belongs</span>
<span style="color: #408080; font-style: italic"># the way we set it up here the index tracks which point and the value which</span>
<span style="color: #408080; font-style: italic"># cluster the point belongs to</span>
cluster_labels <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(n_samples, dtype<span style="color: #666666">=</span><span style="color: #BA2121">&#39;int&#39;</span>)

<span style="color: #408080; font-style: italic"># next we loop through our samples and for every point assign it to the cluster</span>
<span style="color: #408080; font-style: italic"># to which it has the smallest distance to</span>
<span style="color: #008000; font-weight: bold">for</span> n <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_samples):
    <span style="color: #408080; font-style: italic"># tracking variables (all of this is basically just an argmin)</span>
    smallest <span style="color: #666666">=</span> <span style="color: #666666">1e10</span>
    smallest_row_index <span style="color: #666666">=</span> <span style="color: #666666">1e10</span>
    <span style="color: #008000; font-weight: bold">for</span> k <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_clusters):
        <span style="color: #008000; font-weight: bold">if</span> distances[n, k] <span style="color: #666666">&lt;</span> smallest:
            smallest <span style="color: #666666">=</span> distances[n, k]
            smallest_row_index <span style="color: #666666">=</span> k

    cluster_labels[n] <span style="color: #666666">=</span> smallest_row_index
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Let's plot and see</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">fig <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>figure()
ax <span style="color: #666666">=</span> fig<span style="color: #666666">.</span>add_subplot()
unique_cluster_labels <span style="color: #666666">=</span> np<span style="color: #666666">.</span>unique(cluster_labels)
<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> unique_cluster_labels:
    ax<span style="color: #666666">.</span>scatter(data[cluster_labels <span style="color: #666666">==</span> i, <span style="color: #666666">0</span>],
               data[cluster_labels <span style="color: #666666">==</span> i, <span style="color: #666666">1</span>],
               label <span style="color: #666666">=</span> i,
               alpha <span style="color: #666666">=</span> <span style="color: #666666">0.2</span>)
    ax<span style="color: #666666">.</span>scatter(centroids[:, <span style="color: #666666">0</span>], centroids[:, <span style="color: #666666">1</span>], c<span style="color: #666666">=</span><span style="color: #BA2121">&#39;black&#39;</span>)

ax<span style="color: #666666">.</span>set_title(<span style="color: #BA2121">&quot;First Grouping of Points to Centroids&quot;</span>)

plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>So what do we have so far? We have 'picked' \( k \) centroids at random from our
data points. There are other ways of more intelligently choosing their
initializations, however for our purposes randomly is fine. Then we have
initialized an array 'distances' which holds the information of the distance,
<em>or dissimilarity</em>, of every point to of our centroids. Finally, we have
initialized an array 'cluster_labels' which according to our distances array
holds the information of to which centroid every point is assigned. This was the
first pass of our algorithm. Essentially, all we need to do now is repeat the
distance and assignment steps above until we have reached a desired convergence
or a maximum amount of iterations.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">max_iterations <span style="color: #666666">=</span> <span style="color: #666666">100</span>
tolerance <span style="color: #666666">=</span> <span style="color: #666666">1e-8</span>

<span style="color: #008000; font-weight: bold">for</span> iteration <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(max_iterations):
    prev_centroids <span style="color: #666666">=</span> centroids<span style="color: #666666">.</span>copy()
    <span style="color: #008000; font-weight: bold">for</span> k <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_clusters):
        <span style="color: #408080; font-style: italic"># this array will be used to update our centroid positions</span>
        vector_mean <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(dimensions)
        mean_divisor <span style="color: #666666">=</span> <span style="color: #666666">0</span>
        <span style="color: #008000; font-weight: bold">for</span> n <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_samples):
            <span style="color: #008000; font-weight: bold">if</span> cluster_labels[n] <span style="color: #666666">==</span> k:
                vector_mean <span style="color: #666666">+=</span> data[n, :]
                mean_divisor <span style="color: #666666">+=</span> <span style="color: #666666">1</span>

        <span style="color: #408080; font-style: italic"># update according to the k means</span>
        centroids[k, :] <span style="color: #666666">=</span> vector_mean <span style="color: #666666">/</span> mean_divisor

    <span style="color: #408080; font-style: italic"># we find the dissimilarity</span>
    <span style="color: #008000; font-weight: bold">for</span> k <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_clusters):
        <span style="color: #008000; font-weight: bold">for</span> n <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_samples):
            dist <span style="color: #666666">=</span> <span style="color: #666666">0</span>
            <span style="color: #008000; font-weight: bold">for</span> d <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(dimensions):
                dist <span style="color: #666666">+=</span> np<span style="color: #666666">.</span>abs(data[n, d] <span style="color: #666666">-</span> centroids[k, d])<span style="color: #666666">**2</span>
                distances[n, k] <span style="color: #666666">=</span> dist

    <span style="color: #408080; font-style: italic"># assign each point</span>
    <span style="color: #008000; font-weight: bold">for</span> n <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_samples):
        smallest <span style="color: #666666">=</span> <span style="color: #666666">1e10</span>
        smallest_row_index <span style="color: #666666">=</span> <span style="color: #666666">1e10</span>
        <span style="color: #008000; font-weight: bold">for</span> k <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_clusters):
            <span style="color: #008000; font-weight: bold">if</span> distances[n, k] <span style="color: #666666">&lt;</span> smallest:
                smallest <span style="color: #666666">=</span> distances[n, k]
                smallest_row_index <span style="color: #666666">=</span> k

        cluster_labels[n] <span style="color: #666666">=</span> smallest_row_index

    <span style="color: #408080; font-style: italic"># convergence criteria</span>
    centroid_difference <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sum(np<span style="color: #666666">.</span>abs(centroids <span style="color: #666666">-</span> prev_centroids))
    <span style="color: #008000; font-weight: bold">if</span> centroid_difference <span style="color: #666666">&lt;</span> tolerance:
        <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Converged at iteration </span><span style="color: #BB6688; font-weight: bold">{</span>iteration<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&#39;</span>)
        <span style="color: #008000; font-weight: bold">break</span>

    <span style="color: #008000; font-weight: bold">elif</span> iteration <span style="color: #666666">==</span> max_iterations:
        <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Did not converge in </span><span style="color: #BB6688; font-weight: bold">{</span>max_iterations<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121"> iterations&#39;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>And thats it! We now have an extremely barebones, un-optimized k-means
clustering implementation. Lets plot the final result
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">fig <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>figure()
ax <span style="color: #666666">=</span> fig<span style="color: #666666">.</span>add_subplot()
unique_cluster_labels <span style="color: #666666">=</span> np<span style="color: #666666">.</span>unique(cluster_labels)
<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> unique_cluster_labels:
    ax<span style="color: #666666">.</span>scatter(data[cluster_labels <span style="color: #666666">==</span> i, <span style="color: #666666">0</span>],
               data[cluster_labels <span style="color: #666666">==</span> i, <span style="color: #666666">1</span>],
               label <span style="color: #666666">=</span> i,
               alpha <span style="color: #666666">=</span> <span style="color: #666666">0.2</span>)
    ax<span style="color: #666666">.</span>scatter(centroids[:, <span style="color: #666666">0</span>], centroids[:, <span style="color: #666666">1</span>], c<span style="color: #666666">=</span><span style="color: #BA2121">&#39;black&#39;</span>)

ax<span style="color: #666666">.</span>set_title(<span style="color: #BA2121">&quot;Final Result of K-means Clustering&quot;</span>)

plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>And here is an animation of the progression of the algorithm.</p>

<embed src="clustering_example_images/simple_clustering.gif"  autoplay="false" loop="true"></embed>
<p><em></em></p>

<p>The completed code, up to this point, is wrapped in a function for convenience
later.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">naive_kmeans</span>(data, n_clusters<span style="color: #666666">=4</span>, max_iterations<span style="color: #666666">=100</span>, tolerance<span style="color: #666666">=1e-8</span>):
    start_time <span style="color: #666666">=</span> time<span style="color: #666666">.</span>time()

    n_samples, dimensions <span style="color: #666666">=</span> data<span style="color: #666666">.</span>shape
    n_clusters <span style="color: #666666">=</span> <span style="color: #666666">4</span>
    <span style="color: #408080; font-style: italic">#np.random.seed(2021)</span>
    centroids <span style="color: #666666">=</span> data[np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>choice(n_samples, n_clusters, replace<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>), :]
    distances <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((n_samples, n_clusters))

    <span style="color: #008000; font-weight: bold">for</span> k <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_clusters):
        <span style="color: #008000; font-weight: bold">for</span> n <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_samples):
            dist <span style="color: #666666">=</span> <span style="color: #666666">0</span>
            <span style="color: #008000; font-weight: bold">for</span> d <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(dimensions):
                dist <span style="color: #666666">+=</span> np<span style="color: #666666">.</span>abs(data[n, d] <span style="color: #666666">-</span> centroids[k, d])<span style="color: #666666">**2</span>
                distances[n, k] <span style="color: #666666">=</span> dist

    cluster_labels <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(n_samples, dtype<span style="color: #666666">=</span><span style="color: #BA2121">&#39;int&#39;</span>)

    <span style="color: #008000; font-weight: bold">for</span> n <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_samples):
        smallest <span style="color: #666666">=</span> <span style="color: #666666">1e10</span>
        smallest_row_index <span style="color: #666666">=</span> <span style="color: #666666">1e10</span>
        <span style="color: #008000; font-weight: bold">for</span> k <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_clusters):
            <span style="color: #008000; font-weight: bold">if</span> distances[n, k] <span style="color: #666666">&lt;</span> smallest:
                smallest <span style="color: #666666">=</span> distances[n, k]
                smallest_row_index <span style="color: #666666">=</span> k

        cluster_labels[n] <span style="color: #666666">=</span> smallest_row_index

    <span style="color: #008000; font-weight: bold">for</span> iteration <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(max_iterations):
        prev_centroids <span style="color: #666666">=</span> centroids<span style="color: #666666">.</span>copy()
        <span style="color: #008000; font-weight: bold">for</span> k <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_clusters):
            vector_mean <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(dimensions)
            mean_divisor <span style="color: #666666">=</span> <span style="color: #666666">0</span>
            <span style="color: #008000; font-weight: bold">for</span> n <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_samples):
                <span style="color: #008000; font-weight: bold">if</span> cluster_labels[n] <span style="color: #666666">==</span> k:
                    vector_mean <span style="color: #666666">+=</span> data[n, :]
                    mean_divisor <span style="color: #666666">+=</span> <span style="color: #666666">1</span>

            centroids[k, :] <span style="color: #666666">=</span> vector_mean <span style="color: #666666">/</span> mean_divisor

        <span style="color: #008000; font-weight: bold">for</span> k <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_clusters):
            <span style="color: #008000; font-weight: bold">for</span> n <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_samples):
                dist <span style="color: #666666">=</span> <span style="color: #666666">0</span>
                <span style="color: #008000; font-weight: bold">for</span> d <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(dimensions):
                    dist <span style="color: #666666">+=</span> np<span style="color: #666666">.</span>abs(data[n, d] <span style="color: #666666">-</span> centroids[k, d])<span style="color: #666666">**2</span>
                    distances[n, k] <span style="color: #666666">=</span> dist

        <span style="color: #008000; font-weight: bold">for</span> n <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_samples):
            smallest <span style="color: #666666">=</span> <span style="color: #666666">1e10</span>
            smallest_row_index <span style="color: #666666">=</span> <span style="color: #666666">1e10</span>
            <span style="color: #008000; font-weight: bold">for</span> k <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_clusters):
                <span style="color: #008000; font-weight: bold">if</span> distances[n, k] <span style="color: #666666">&lt;</span> smallest:
                    smallest <span style="color: #666666">=</span> distances[n, k]
                    smallest_row_index <span style="color: #666666">=</span> k

            cluster_labels[n] <span style="color: #666666">=</span> smallest_row_index

        centroid_difference <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sum(np<span style="color: #666666">.</span>abs(centroids <span style="color: #666666">-</span> prev_centroids))
        <span style="color: #008000; font-weight: bold">if</span> centroid_difference <span style="color: #666666">&lt;</span> tolerance:
            <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Converged at iteration </span><span style="color: #BB6688; font-weight: bold">{</span>iteration<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&#39;</span>)
            <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Runtime: </span><span style="color: #BB6688; font-weight: bold">{</span>time<span style="color: #666666">.</span>time() <span style="color: #666666">-</span> start_time<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121"> seconds&#39;</span>)

            <span style="color: #008000; font-weight: bold">return</span> cluster_labels, centroids

    <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Did not converge in </span><span style="color: #BB6688; font-weight: bold">{</span>max_iterations<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121"> iterations&#39;</span>)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Runtime: </span><span style="color: #BB6688; font-weight: bold">{</span>time<span style="color: #666666">.</span>time() <span style="color: #666666">-</span> start_time<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121"> seconds&#39;</span>)

    <span style="color: #008000; font-weight: bold">return</span> cluster_labels, centroids
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Now there are a few glaring improvements to be done here. First of all is
organizing things into functions for better readability. Second is getting rid
of the small inefficiencies like manually calculating distances and argmin. And
finally, we need to optimize for better run-time. It's like we always say: the
best way of looping in Python is to not loop in Python. Let us tackle the first
two improvements.
</p>
<h3 id="towards-a-more-numpythonic-code">Towards a More Numpythonic Code </h3>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">get_distances_to_clusters</span>(data, centroids):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Function that for each cluster finds the squared Euclidean distance</span>
<span style="color: #BA2121; font-style: italic">    from every data point to the cluster centroid and returns a numpy array</span>
<span style="color: #BA2121; font-style: italic">    containing the distances such that distance[i, j] means the distance between</span>
<span style="color: #BA2121; font-style: italic">    the i-th point and the j-th centroid.</span>
<span style="color: #BA2121; font-style: italic">    Inputs:</span>
<span style="color: #BA2121; font-style: italic">        data (np.array): with dimensions (n_samples x dim)</span>
<span style="color: #BA2121; font-style: italic">        centroids (np.array): with dimensions (n_clusters x dim)</span>

<span style="color: #BA2121; font-style: italic">    Returns:</span>
<span style="color: #BA2121; font-style: italic">        distances (np.array): with dimensions (n_samples x n_clusters)</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>

    n_samples, dimensions <span style="color: #666666">=</span> data<span style="color: #666666">.</span>shape
    n_clusters <span style="color: #666666">=</span> centroids<span style="color: #666666">.</span>shape[<span style="color: #666666">0</span>]
    distances <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((n_samples, n_clusters))
    <span style="color: #008000; font-weight: bold">for</span> k <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_clusters):
        <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_samples):
            dist <span style="color: #666666">=</span> <span style="color: #666666">0</span>
            <span style="color: #008000; font-weight: bold">for</span> j <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(dimensions):
                dist <span style="color: #666666">+=</span> np<span style="color: #666666">.</span>abs(data[i, j] <span style="color: #666666">-</span> centroids[k, j])<span style="color: #666666">**2</span>
                distances[i, k] <span style="color: #666666">=</span> dist

    <span style="color: #008000; font-weight: bold">return</span> distances



<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">assign_points_to_clusters</span>(distances):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Function to assign each data point to the cluster to which it is the closest</span>
<span style="color: #BA2121; font-style: italic">    based on the squared Euclidean distance from the get_distances_to_clusters</span>
<span style="color: #BA2121; font-style: italic">    method.</span>
<span style="color: #BA2121; font-style: italic">    Inputs:</span>
<span style="color: #BA2121; font-style: italic">        distances (np.array): with dimensions (n_samples x n_clusters)</span>

<span style="color: #BA2121; font-style: italic">    Returns:</span>
<span style="color: #BA2121; font-style: italic">        cluster_labels (np.array): with dimensions (n_samples)</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    cluster_labels <span style="color: #666666">=</span> np<span style="color: #666666">.</span>argmin(distances, axis<span style="color: #666666">=1</span>)

    <span style="color: #008000; font-weight: bold">return</span> cluster_labels



<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">k_means</span>(data, n_clusters<span style="color: #666666">=4</span>, max_iterations<span style="color: #666666">=100</span>, tolerance<span style="color: #666666">=1e-8</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Naive implementation of the k-means clustering algorithm. A short summary of</span>
<span style="color: #BA2121; font-style: italic">    the algorithm is as follows: we randomly initialize k centroids / means.</span>
<span style="color: #BA2121; font-style: italic">    Then we assign, using the squared Euclidean distance, every data-point to a</span>
<span style="color: #BA2121; font-style: italic">    cluster. We then update the position of the k centroids / means, and repeat</span>
<span style="color: #BA2121; font-style: italic">    until convergence or we reach our desired maximum iterations. The method</span>
<span style="color: #BA2121; font-style: italic">    returns the cluster assignments of our data-points and a sequence of</span>
<span style="color: #BA2121; font-style: italic">    centroids.</span>
<span style="color: #BA2121; font-style: italic">    Inputs:</span>
<span style="color: #BA2121; font-style: italic">        data (np.array): with dimesions (n_samples x dim)</span>
<span style="color: #BA2121; font-style: italic">        n_clusters (int): hyperparameter which depends on dataset</span>
<span style="color: #BA2121; font-style: italic">        max_iterations (int): hyperparameter which depends on dataset</span>
<span style="color: #BA2121; font-style: italic">        tolerance (float): convergence measure</span>

<span style="color: #BA2121; font-style: italic">    Returns:</span>
<span style="color: #BA2121; font-style: italic">        cluster_labels (np.array): with dimension (n_samples)</span>
<span style="color: #BA2121; font-style: italic">        centroid_list (list): list of centroids (np.array)</span>
<span style="color: #BA2121; font-style: italic">                              with dimensions (n_clusters x dim)</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    start_time <span style="color: #666666">=</span> time<span style="color: #666666">.</span>time()
    n_samples, dimensions <span style="color: #666666">=</span> data<span style="color: #666666">.</span>shape
    <span style="color: #408080; font-style: italic">#np.random.seed(2021)</span>
    centroids <span style="color: #666666">=</span> data[np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>choice(<span style="color: #008000">len</span>(data), n_clusters, replace<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>), :]
    distances <span style="color: #666666">=</span> get_distances_to_clusters(data, centroids)
    cluster_labels <span style="color: #666666">=</span> assign_points_to_clusters(distances)


    <span style="color: #008000; font-weight: bold">for</span> iteration <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(max_iterations):
        prev_centroids <span style="color: #666666">=</span> centroids<span style="color: #666666">.</span>copy()
        <span style="color: #008000; font-weight: bold">for</span> k <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_clusters):
            vector_mean <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(dimensions)
            mean_divisor <span style="color: #666666">=</span> <span style="color: #666666">0</span>
            <span style="color: #008000; font-weight: bold">for</span> n <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_samples):
                <span style="color: #008000; font-weight: bold">if</span> cluster_labels[n] <span style="color: #666666">==</span> k:
                    vector_mean <span style="color: #666666">+=</span> data[n, :]
                    mean_divisor <span style="color: #666666">+=</span> <span style="color: #666666">1</span>
            <span style="color: #408080; font-style: italic"># And update according to the new means</span>
            centroids[k, :] <span style="color: #666666">=</span> vector_mean <span style="color: #666666">/</span> mean_divisor

        distances <span style="color: #666666">=</span> get_distances_to_clusters(data, centroids)
        cluster_labels <span style="color: #666666">=</span> assign_points_to_clusters(distances)

        centroid_difference <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sum(np<span style="color: #666666">.</span>abs(centroids <span style="color: #666666">-</span> prev_centroids))
        <span style="color: #008000; font-weight: bold">if</span> centroid_difference <span style="color: #666666">&lt;</span> tolerance:
            <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Converged at iteration: </span><span style="color: #BB6688; font-weight: bold">{</span>iteration<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&#39;</span>)
            <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Runtime: </span><span style="color: #BB6688; font-weight: bold">{</span>time<span style="color: #666666">.</span>time() <span style="color: #666666">-</span> start_time<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121"> seconds&#39;</span>)

            <span style="color: #008000; font-weight: bold">return</span> cluster_labels, centroids

    <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Did not converge in </span><span style="color: #BB6688; font-weight: bold">{</span>max_iterations<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121"> iterations&#39;</span>)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Runtime: </span><span style="color: #BB6688; font-weight: bold">{</span>time<span style="color: #666666">.</span>time() <span style="color: #666666">-</span> start_time<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121"> seconds&#39;</span>)

    <span style="color: #008000; font-weight: bold">return</span> cluster_labels, centroids

cluster_labels, centroids <span style="color: #666666">=</span> k_means(data)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Notice we added timing to our code. Although, the result of timing once will not
be optimal, it can still give us hints about the order of magnitude of our
subsequent improvements
</p>

<p>So we see an improvement from just switching to numpy's argmin function. There
is a very nice tool (or category of tools) called profilers. These can be
utilized to make clearer which improvements to our code we should care most
about here is an <a href="https://ipython-books.github.io/42-profiling-your-code-easily-with-cprofile-and-ipython/" target="_self">excellent source</a>
on the topic. Even before optimizing we can understand which parts of our code
will be taking the most of the run-time. It will be the longest Python loop,
i.e. the loop over all the samples. Nonetheless, let us do some profiling!
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">test_data <span style="color: #666666">=</span> generate_simple_clustering_dataset(n_points<span style="color: #666666">=10000</span>, plotting<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)
<span style="color: #666666">%</span>prun <span style="color: #666666">-</span>l <span style="color: #666666">10</span> cluster_labels, centroids <span style="color: #666666">=</span> k_means(test_data)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Here we can see the reason for profiling. We now know for certain a lot can be
gained just by vectorizing our distance function. Ideally we wish to perform
most of our loops in numpy, i.e. C. To do this we need our array shapes to match
and clever reshaping will let us do so.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">np_get_distances_to_clusters</span>(data, centroids):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Squared Euclidean distance between all data-points and every centroid. For</span>
<span style="color: #BA2121; font-style: italic">    the function to work properly it needs data and centroids to be numpy</span>
<span style="color: #BA2121; font-style: italic">    broadcastable. We sum along the dimension axis.</span>
<span style="color: #BA2121; font-style: italic">    Inputs:</span>
<span style="color: #BA2121; font-style: italic">        data (np.array): with dimensions (samples x 1 x dim)</span>
<span style="color: #BA2121; font-style: italic">        centroids (np.array): with dimensions (1 x n_clusters x dim)</span>

<span style="color: #BA2121; font-style: italic">    Returns:</span>
<span style="color: #BA2121; font-style: italic">        distances (np.array): with dimensions (samples x n_clusters)</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>

    distances <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sum(np<span style="color: #666666">.</span>abs((data <span style="color: #666666">-</span> centroids))<span style="color: #666666">**2</span>, axis<span style="color: #666666">=2</span>)
    <span style="color: #008000; font-weight: bold">return</span> distances

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">np_assign_points_to_clusters</span>(distances):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Assigning each data-point to a cluster given an array distances containing</span>
<span style="color: #BA2121; font-style: italic">    the squared Euclidean distance from every point to each centroid. We do</span>
<span style="color: #BA2121; font-style: italic">    np.argmin along the cluster axis to find the closest cluster. Returns a</span>
<span style="color: #BA2121; font-style: italic">    numpy array with corresponding labels.</span>
<span style="color: #BA2121; font-style: italic">    Inputs:</span>
<span style="color: #BA2121; font-style: italic">        distances (np.array): with dimensions (samples x n_clusters)</span>

<span style="color: #BA2121; font-style: italic">    Returns:</span>
<span style="color: #BA2121; font-style: italic">        cluster_labels (np.array): with dimensions (samples x None)</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    cluster_labels <span style="color: #666666">=</span> np<span style="color: #666666">.</span>argmin(distances, axis<span style="color: #666666">=1</span>)
    <span style="color: #008000; font-weight: bold">return</span> cluster_labels


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">np_k_means</span>(data, n_clusters<span style="color: #666666">=4</span>, max_iterations<span style="color: #666666">=100</span>, tolerance<span style="color: #666666">=1e-8</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Numpythonic implementation of the k-means clusting algorithm.</span>
<span style="color: #BA2121; font-style: italic">    Inputs:</span>
<span style="color: #BA2121; font-style: italic">        data (np.array): with dimesions (n_samples x dim)</span>
<span style="color: #BA2121; font-style: italic">        n_clusters (int): hyperparameter which depends on dataset</span>
<span style="color: #BA2121; font-style: italic">        max_iterations (int): hyperparameter which depends on dataset</span>
<span style="color: #BA2121; font-style: italic">        tolerance (float): convergence measure</span>

<span style="color: #BA2121; font-style: italic">    Returns:</span>
<span style="color: #BA2121; font-style: italic">        cluster_labels (np.array): with dimension (n_samples)</span>
<span style="color: #BA2121; font-style: italic">        centroid_list (list): list of centroids (np.array)</span>
<span style="color: #BA2121; font-style: italic">                              with dimensions (n_clusters x dim)</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    start_time <span style="color: #666666">=</span> time<span style="color: #666666">.</span>time()
    n_samples, dimensions <span style="color: #666666">=</span> data<span style="color: #666666">.</span>shape
    <span style="color: #408080; font-style: italic">#np.random.seed(2021)</span>
    centroids <span style="color: #666666">=</span> data[np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>choice(<span style="color: #008000">len</span>(data), n_clusters, replace<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>), :]

    distances <span style="color: #666666">=</span> np_get_distances_to_clusters(np<span style="color: #666666">.</span>reshape(data,
                                            (n_samples, <span style="color: #666666">1</span>, dimensions)),
                                          np<span style="color: #666666">.</span>reshape(centroids,
                                            (<span style="color: #666666">1</span>, n_clusters, dimensions)))
    cluster_labels <span style="color: #666666">=</span> np_assign_points_to_clusters(distances)


    <span style="color: #008000; font-weight: bold">for</span> iteration <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(max_iterations):
        prev_centroids <span style="color: #666666">=</span> centroids<span style="color: #666666">.</span>copy()
        <span style="color: #008000; font-weight: bold">for</span> k <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_clusters):
            points_in_cluster <span style="color: #666666">=</span> data[cluster_labels <span style="color: #666666">==</span> k]
            mean_vector <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean(points_in_cluster, axis<span style="color: #666666">=0</span>)
            centroids[k] <span style="color: #666666">=</span> mean_vector

        distances <span style="color: #666666">=</span> np_get_distances_to_clusters(np<span style="color: #666666">.</span>reshape(data,
                                                (n_samples, <span style="color: #666666">1</span>, dimensions)),
                                              np<span style="color: #666666">.</span>reshape(centroids,
                                                (<span style="color: #666666">1</span>, n_clusters, dimensions)))
        cluster_labels <span style="color: #666666">=</span> np_assign_points_to_clusters(distances)

        centroid_difference <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sum(np<span style="color: #666666">.</span>abs(centroids <span style="color: #666666">-</span> prev_centroids))
        <span style="color: #008000; font-weight: bold">if</span> centroid_difference <span style="color: #666666">&lt;</span> tolerance:
            <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Converged at iteration: </span><span style="color: #BB6688; font-weight: bold">{</span>iteration<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&#39;</span>)
            <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Runtime: </span><span style="color: #BB6688; font-weight: bold">{</span>time<span style="color: #666666">.</span>time() <span style="color: #666666">-</span> start_time<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121"> seconds&#39;</span>)

            <span style="color: #008000; font-weight: bold">return</span> cluster_labels, centroids

    <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Did not converge in </span><span style="color: #BB6688; font-weight: bold">{</span>max_iterations<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121"> iterations&#39;</span>)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Runtime: </span><span style="color: #BB6688; font-weight: bold">{</span>time<span style="color: #666666">.</span>time() <span style="color: #666666">-</span> start_time<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121"> seconds&#39;</span>)

    <span style="color: #008000; font-weight: bold">return</span> cluster_labels, centroids
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>When working towards becoming a data scientist using Python this last step is
arguably one of the most important. Thinking of ways to avoid explicitly looping
by adding dimensions to our arrays in such a way that they become broadcastable
using numpy (also tensorflow and many others). Let us take a look at our the
fruits of our labor.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">cluster_labels, centroids <span style="color: #666666">=</span> np_k_means(data)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>So our new code is around <b>two orders of magnitude</b> faster on this limited test
example!
</p>

<p>So let us recap what we have learned. The \( k \)-means algorithm works by
iteratively updating \( k \) means according to the squared Euclidean distance to
our data points. One way of thinking about it is in terms of compressing the
original high dimensional data to a lower dimension. The process of which,
<em>hopefully</em>, captures something about the structure of the original data.
Phrased according to our example, we start of with 4000 '*groups*' which we
'compress' to just four '*groups*'. We have first hand seen the unreasonable
inefficiency of pure Python loops and learned to think about loops as just
another way of looking at the dimensionality of our data.
</p>
<h3 id="onto-bigger-and-better-things">Onto Bigger and Better Things </h3>
<p>As is often the case, someone has already done everything we have just done
(often but not always better). Let us take a look at <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" target="_self">scikit-learn's
implementation</a>.
We define a wrapper function for timing and benchmarking purposes later.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">skl_kmeans</span>(data, n_clusters<span style="color: #666666">=4</span>, max_iterations<span style="color: #666666">=100</span>, tolerance<span style="color: #666666">=1e-8</span>):
    start_time <span style="color: #666666">=</span> time<span style="color: #666666">.</span>time()
    <span style="color: #408080; font-style: italic">#np.random.seed(2021)</span>
    kmeans <span style="color: #666666">=</span> KMeans(n_clusters<span style="color: #666666">=</span>n_clusters, max_iter<span style="color: #666666">=</span>max_iterations,
                tol<span style="color: #666666">=</span>tolerance)<span style="color: #666666">.</span>fit(data)

    <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Converged at iteration </span><span style="color: #BB6688; font-weight: bold">{</span>kmeans<span style="color: #666666">.</span>n_iter_<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&#39;</span>)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Runtime: </span><span style="color: #BB6688; font-weight: bold">{</span>time<span style="color: #666666">.</span>time() <span style="color: #666666">-</span> start_time<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121"> seconds&#39;</span>)

    <span style="color: #008000; font-weight: bold">return</span> kmeans<span style="color: #666666">.</span>labels_, kmeans<span style="color: #666666">.</span>cluster_centers_
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Now let us get an idea how we hold up against the professionals.</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">cluster_labels, centroids <span style="color: #666666">=</span> skl_kmeans(data)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Here we get another important lesson, there is a cost for convenience. In
addition, there might be various tests and other things enforcing robustness in
the sklearn implementation which are not present in our numpy code. However,
the result still speaks for themselves.
</p>

<p>The final implementation we will look at is one in tensorflow. Now while
tensorflow is not strictly meant for things like this. It is natively highly
parallelized with functionality out of the box to run on GPU's. The following
code is based on, and adapted from this <a href="https://www.altoros.com/blog/using-k-means-clustering-in-tensorflow/" target="_self">blog-post</a>
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">tf_kmeans</span>(data, n_clusters<span style="color: #666666">=4</span>, max_iterations<span style="color: #666666">=100</span>, tolerance<span style="color: #666666">=1e-8</span>):
    start_time <span style="color: #666666">=</span> time<span style="color: #666666">.</span>time()
    data <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>constant(data)
    centroids <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>constant(tf<span style="color: #666666">.</span>slice(tf<span style="color: #666666">.</span>random<span style="color: #666666">.</span>shuffle(data),
                              begin<span style="color: #666666">=</span>[<span style="color: #666666">0</span>, <span style="color: #666666">0</span>],
                              size<span style="color: #666666">=</span>[n_clusters, <span style="color: #666666">-1</span>]))

    <span style="color: #AA22FF">@tf</span><span style="color: #666666">.</span>function
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">update_centroids</span>(data, centroids):
        data_expanded <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>expand_dims(data, <span style="color: #666666">0</span>)
        centroids_expanded <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>expand_dims(centroids, <span style="color: #666666">1</span>)

        distances <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>reduce_sum(
                        tf<span style="color: #666666">.</span>square(
                            tf<span style="color: #666666">.</span>subtract(data_expanded, centroids_expanded)), <span style="color: #666666">2</span>)
        cluster_labels <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>argmin(distances, <span style="color: #666666">0</span>)
        means <span style="color: #666666">=</span> []

        <span style="color: #008000; font-weight: bold">for</span> k <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_clusters):
            temp <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>reshape(tf<span style="color: #666666">.</span>where(tf<span style="color: #666666">.</span>equal(cluster_labels, k)),
                      shape<span style="color: #666666">=</span>[<span style="color: #666666">1</span>, <span style="color: #666666">-1</span>])
            temp <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>gather(data, temp, validate_indices<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>)
            temp <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>reduce_mean(temp, axis<span style="color: #666666">=</span>[<span style="color: #666666">1</span>])
            means<span style="color: #666666">.</span>append(temp)
            updated_centroids <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>concat(means, <span style="color: #666666">0</span>)

        <span style="color: #008000; font-weight: bold">return</span> cluster_labels, updated_centroids

    <span style="color: #008000; font-weight: bold">for</span> iteration <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(max_iterations):
        prev_centroids <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>identity(centroids)
        cluster_labels, centroids <span style="color: #666666">=</span> update_centroids(data, centroids)

        <span style="color: #008000; font-weight: bold">if</span> tf<span style="color: #666666">.</span>reduce_sum(
            tf<span style="color: #666666">.</span>abs(
                tf<span style="color: #666666">.</span>subtract(prev_centroids, centroids))) <span style="color: #666666">&lt;</span> tolerance:

            <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Converged at iteration </span><span style="color: #BB6688; font-weight: bold">{</span>iteration<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&#39;</span>)
            <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Runtime: </span><span style="color: #BB6688; font-weight: bold">{</span>time<span style="color: #666666">.</span>time() <span style="color: #666666">-</span> start_time<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121"> seconds&#39;</span>)

            <span style="color: #008000; font-weight: bold">return</span> cluster_labels, centroids

    <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Did not converge in </span><span style="color: #BB6688; font-weight: bold">{</span>max_iterations<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121"> iterations&#39;</span>)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Runtime: </span><span style="color: #BB6688; font-weight: bold">{</span>time<span style="color: #666666">.</span>time() <span style="color: #666666">-</span> start_time<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121"> seconds&#39;</span>)
    <span style="color: #008000; font-weight: bold">return</span> cluster_labels, centroids
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">cluster_labels, centroids <span style="color: #666666">=</span> tf_kmeans(data)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p><b>Note</b> tensorflow has a different seed than numpy, so direct comparison of just
one run of the code (bad practice) is not doable. Also notice the @tf.function.
This was introduced in tensorflow 2 and signifies that the function is
'compiled'. We will look at benchmarking all our methods next.
</p>

<p>But first we plot or result to see we get something which looks correct. We also
take this opportunity to define a function for our plotting.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">make_plot</span>(data, cluster_labels, centroids, method_string):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Simple plot function</span>

<span style="color: #BA2121; font-style: italic">    Inputs:</span>
<span style="color: #BA2121; font-style: italic">        data (np.array like) with dimensions: (n_samples x dim)</span>
<span style="color: #BA2121; font-style: italic">        cluster_labels (np.array like) with dimensions: (n_samples)</span>
<span style="color: #BA2121; font-style: italic">        centroids (np.array like) with dimensions: (n_clusters x dim)</span>

<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    fig <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>figure()
    ax <span style="color: #666666">=</span> fig<span style="color: #666666">.</span>add_subplot()
    unique_cluster_labels <span style="color: #666666">=</span> np<span style="color: #666666">.</span>unique(cluster_labels)
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> unique_cluster_labels:
        ax<span style="color: #666666">.</span>scatter(data[cluster_labels <span style="color: #666666">==</span> i, <span style="color: #666666">0</span>],
                   data[cluster_labels <span style="color: #666666">==</span> i, <span style="color: #666666">1</span>],
                   label <span style="color: #666666">=</span> i,
                   alpha <span style="color: #666666">=</span> <span style="color: #666666">0.2</span>)
        ax<span style="color: #666666">.</span>scatter(centroids[:, <span style="color: #666666">0</span>], centroids[:, <span style="color: #666666">1</span>], c<span style="color: #666666">=</span><span style="color: #BA2121">&#39;black&#39;</span>)

    ax<span style="color: #666666">.</span>set_title(<span style="color: #BA2121">f&quot;K Means Clustering using </span><span style="color: #BB6688; font-weight: bold">{</span>method_string<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121"> Method&quot;</span>)

    plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">make_plot(data, cluster_labels, centroids, <span style="color: #BA2121">&quot;Tensorflow&quot;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Looks good! To summarize, we have now seen how to code the most basic Python
version, how to optimize and use a profiler, we ended up with a really quick
numpythonic version of our code. Then we looked at two implementations of the
\( k \) means algorithm using higher level libraries scikit-learn and tensorflow.
</p>
<h2 id="benchmarking-and-testing-our-code">Benchmarking and Testing Our Code </h2>
<p>Now that we have developed our code, the next logical step is testing how well
our different implementations fare against each other in various tests. We start
with doing a Monte Carlo run over all our methods to produce numerical estimates
for the performance of our implementations.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">monte_carlo_cycles <span style="color: #666666">=</span> <span style="color: #666666">10</span>
n_data_points <span style="color: #666666">=</span> np<span style="color: #666666">.</span>logspace(<span style="color: #666666">10</span>, <span style="color: #666666">100</span>, num<span style="color: #666666">=2</span>)
method_dict <span style="color: #666666">=</span> {
    <span style="color: #BA2121">&#39;Python&#39;</span>: naive_kmeans,
    <span style="color: #BA2121">&#39;Numpy&#39;</span>: np_k_means,
    <span style="color: #BA2121">&#39;Scikit-Learn&#39;</span>: skl_kmeans,
    <span style="color: #BA2121">&#39;Tensorflow&#39;</span>: tf_kmeans
}
times_array <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((<span style="color: #666666">4</span>, <span style="color: #008000">len</span>(n_data_points), monte_carlo_cycles))

<span style="color: #008000; font-weight: bold">for</span> i, (method_string, method) <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(method_dict<span style="color: #666666">.</span>items()):
    <span style="color: #008000; font-weight: bold">for</span> j, n_points <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(n_data_points):
        data <span style="color: #666666">=</span> generate_simple_clustering_dataset(n_points<span style="color: #666666">=</span>n_points,
                                                  plotting<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)

        <span style="color: #008000; font-weight: bold">for</span> k <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(monte_carlo_cycles):
            toc <span style="color: #666666">=</span> time<span style="color: #666666">.</span>time()
            cluster_labels, centroids <span style="color: #666666">=</span> method(data)
            tic <span style="color: #666666">=</span> time<span style="color: #666666">.</span>time() <span style="color: #666666">-</span> toc
            times_array[i, j, k] <span style="color: #666666">=</span> tic
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">means <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean(times_array, axis<span style="color: #666666">=2</span>)
std <span style="color: #666666">=</span> np<span style="color: #666666">.</span>std(times_array, axis<span style="color: #666666">=2</span>)

fig <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>figure()
ax <span style="color: #666666">=</span> fig<span style="color: #666666">.</span>add_subplot()

<span style="color: #008000; font-weight: bold">for</span> i, method_string <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(method_dict):
    <span style="color: #008000">print</span>(i, method_string)
    ax<span style="color: #666666">.</span>errorbar(n_data_points, means[i, :], yerr<span style="color: #666666">=</span>std[i, :],
                label<span style="color: #666666">=</span>method_string,
                markersize<span style="color: #666666">=10</span>,
                capsize<span style="color: #666666">=5</span>)

ax<span style="color: #666666">.</span>set_xlabel(<span style="color: #BA2121">&#39;$n$ data points&#39;</span>)
ax<span style="color: #666666">.</span>set_ylabel(<span style="color: #BA2121">&#39;$t$ seconds&#39;</span>)
fig<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>The naive Python implementation is too slow to see the nuance in the others, we
plot again without it
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">fig <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>figure()
ax <span style="color: #666666">=</span> fig<span style="color: #666666">.</span>add_subplot()

<span style="color: #408080; font-style: italic"># this turned out a bit hacky</span>
<span style="color: #008000; font-weight: bold">for</span> i, method_string <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(<span style="color: #008000">list</span>(method_dict<span style="color: #666666">.</span>keys())[<span style="color: #666666">1</span>:]):
    ax<span style="color: #666666">.</span>errorbar(n_data_points, means[i<span style="color: #666666">+1</span>, :], yerr<span style="color: #666666">=</span>std[i<span style="color: #666666">+1</span>, :],
                label<span style="color: #666666">=</span>method_string,
                markersize<span style="color: #666666">=10</span>,
                capsize<span style="color: #666666">=5</span>)

ax<span style="color: #666666">.</span>set_xlabel(<span style="color: #BA2121">&#39;$n$ data points&#39;</span>)
ax<span style="color: #666666">.</span>set_ylabel(<span style="color: #BA2121">&#39;$t$ seconds&#39;</span>)
fig<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<b>THERE NEEDS TO BE A DISCUSSION OF RESULTS HERE WHEN I PRODUCED THEM</b>

<b>POTENTIALLY NEED TO DO SOMETHING LIKE THE SKL CLUSTERING EXAMPLE</b>
<b>THAT COULD LEAD TO DISCUSSION OF STRENGTHS AND WEAKNESSES OF METHOD</b>
<h2 id="vector-quantization-actual-example-use-case">Vector Quantization: Actual Example Use Case </h2>
<p>Before we wrap up this topic we will consider the topic of <b>vector quantization</b>
. It is a technique which comes from signal processing theory. And is a basis
for some kinds of lossy compression. This is obvious because the \( k \)-means
algorithm is non-injective (many-to-one). Take notice of the fact that while the
\( k \)-means algorithm is an example of a vector quantization, there also exists
other vector quantization algorithms. We will be doing compression on an image,
but in principle this method can be applied to any form of data. We leave it to
the interested reader to try doing compression on something else.
</p>

<p>We load in our image. And do some array manipulation to make the image size more
manageable. The way this will work is our centroids will represent colors, and
clustering means that we 'force' pixels that originally were not that color to
'turn' that color. Some cool art can probably be made using this. Do not
hesitate to try for yourself on another image!!
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">example_image <span style="color: #666666">=</span> image<span style="color: #666666">.</span>imread(<span style="color: #BA2121">&#39;clustering_example_images/some_image.jpg&#39;</span>) <span style="color: #666666">/</span> <span style="color: #666666">255</span>
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Initial shape: </span><span style="color: #BB6688; font-weight: bold">{</span>example_image<span style="color: #666666">.</span>shape<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&#39;</span>)
x, y, rgb <span style="color: #666666">=</span> example_image<span style="color: #666666">.</span>shape
example_image <span style="color: #666666">=</span> example_image[<span style="color: #666666">0</span>:<span style="color: #666666">500</span>, <span style="color: #666666">200</span>:<span style="color: #666666">800</span>, :]
reshaped_image <span style="color: #666666">=</span> np<span style="color: #666666">.</span>reshape(example_image, (<span style="color: #666666">500</span> <span style="color: #666666">*</span> <span style="color: #666666">600</span>, rgb))
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;New shape: </span><span style="color: #BB6688; font-weight: bold">{</span>reshaped_image<span style="color: #666666">.</span>shape<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&#39;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<br/><br/>
<center>
<p><img src="clustering_example_images/some_image.jpg" ></p>
</center>
<br/><br/>

<p>Let's first try clustering with different numbers of centroids and look at the
effect. We will use the numpy implementation because it was the fastest.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">cluster_labels_4, centroids_4 <span style="color: #666666">=</span> np_k_means(reshaped_image, n_clusters<span style="color: #666666">=4</span>,
                                            tolerance<span style="color: #666666">=1e-4</span>)
cluster_labels_6, centroids_6 <span style="color: #666666">=</span> np_k_means(reshaped_image, n_clusters<span style="color: #666666">=6</span>,
                                            tolerance<span style="color: #666666">=1e-4</span>)
cluster_labels_8, centroids_8 <span style="color: #666666">=</span> np_k_means(reshaped_image, n_clusters<span style="color: #666666">=8</span>,
                                            tolerance<span style="color: #666666">=1e-4</span>)

compressed_image_4 <span style="color: #666666">=</span> reshaped_image<span style="color: #666666">.</span>copy()
compressed_image_6 <span style="color: #666666">=</span> reshaped_image<span style="color: #666666">.</span>copy()
compressed_image_8 <span style="color: #666666">=</span> reshaped_image<span style="color: #666666">.</span>copy()

<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #008000">len</span>(reshaped_image)):
    compressed_image_4[i] <span style="color: #666666">=</span> centroids_4[cluster_labels_4[i]]
    compressed_image_6[i] <span style="color: #666666">=</span> centroids_6[cluster_labels_6[i]]
    compressed_image_8[i] <span style="color: #666666">=</span> centroids_8[cluster_labels_8[i]]
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">fig, ax <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplots(<span style="color: #666666">2</span>, <span style="color: #666666">2</span>, figsize<span style="color: #666666">=</span>(<span style="color: #666666">8</span>, <span style="color: #666666">5</span>))
ax[<span style="color: #666666">0</span>, <span style="color: #666666">0</span>]<span style="color: #666666">.</span>imshow(example_image)
ax[<span style="color: #666666">0</span>, <span style="color: #666666">0</span>]<span style="color: #666666">.</span>set_title(<span style="color: #BA2121">&#39;Original&#39;</span>)
ax[<span style="color: #666666">0</span>, <span style="color: #666666">0</span>]<span style="color: #666666">.</span>axis(<span style="color: #BA2121">&#39;off&#39;</span>)

ax[<span style="color: #666666">0</span>, <span style="color: #666666">1</span>]<span style="color: #666666">.</span>imshow(np<span style="color: #666666">.</span>reshape(compressed_image_4, (<span style="color: #666666">500</span>, <span style="color: #666666">600</span>, <span style="color: #666666">3</span>)))
ax[<span style="color: #666666">0</span>, <span style="color: #666666">1</span>]<span style="color: #666666">.</span>set_title(<span style="color: #BA2121">&#39;$n_c = 4$&#39;</span>)
ax[<span style="color: #666666">0</span>, <span style="color: #666666">1</span>]<span style="color: #666666">.</span>axis(<span style="color: #BA2121">&#39;off&#39;</span>)

ax[<span style="color: #666666">1</span>, <span style="color: #666666">0</span>]<span style="color: #666666">.</span>imshow(np<span style="color: #666666">.</span>reshape(compressed_image_6, (<span style="color: #666666">500</span>, <span style="color: #666666">600</span>, <span style="color: #666666">3</span>)))
ax[<span style="color: #666666">1</span>, <span style="color: #666666">0</span>]<span style="color: #666666">.</span>set_title(<span style="color: #BA2121">&#39;$n_c = 6$&#39;</span>)
ax[<span style="color: #666666">1</span>, <span style="color: #666666">0</span>]<span style="color: #666666">.</span>axis(<span style="color: #BA2121">&#39;off&#39;</span>)

ax[<span style="color: #666666">1</span>, <span style="color: #666666">1</span>]<span style="color: #666666">.</span>imshow(np<span style="color: #666666">.</span>reshape(compressed_image_8, (<span style="color: #666666">500</span>, <span style="color: #666666">600</span>, <span style="color: #666666">3</span>)))
ax[<span style="color: #666666">1</span>, <span style="color: #666666">1</span>]<span style="color: #666666">.</span>set_title(<span style="color: #BA2121">&#39;$n_c = 8$&#39;</span>)
ax[<span style="color: #666666">1</span>, <span style="color: #666666">1</span>]<span style="color: #666666">.</span>axis(<span style="color: #BA2121">&#39;off&#39;</span>)

plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>And we have randomly stumbled upon how stencils for spray painting can be made.
Definitely a lot of potential for cool art here!
</p>
<!-- ------------------- end of main content --------------- -->
</body>
</html>

