{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2829f4b0",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html Clustering.do.txt  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fb7c55",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Clustering Analysis\n",
    "In this chapter we will concern ourselves with the study of **cluster analysis**.\n",
    "In general terms cluster analysis, or clustering, is the task of grouping a\n",
    "data-set into different distinct categories based on some measure of equality of\n",
    "the data. This measure is often referred to as a **metric** or **similarity\n",
    "measure** in the literature (note: sometimes we deal with a **dissimilarity\n",
    "measure** instead). Usually, these metrics are formulated as some kind of\n",
    "distance function between points in a high-dimensional space.\n",
    "\n",
    "There exists a lot of such distance measures. The simplest, and also the most\n",
    "common is the **Euclidean distance** (i.e. Pythagoras). A good source for those of\n",
    "you wanting a thorough overview is the article (DOI:10.5120/ijca2016907841\n",
    "Irani, Pise, Phatak). A few other metrics mentioned there are: *cosine\n",
    "similarity*, *Manhattan distance*, *Chebychev distance* and the *Minkowski\n",
    "distance*. The Minkowski distance is a general formulation which encapsulates a\n",
    "range of metrics. All of these, and many more, can be used in clustering. There\n",
    "exists different categories of clustering algorithms. A few of the most\n",
    "common are: *centroid-*, *distribution-*, *density-* and *hierarchical-\n",
    "clustering*. We will concern ourselves primarily with the first one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81a28ab",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Basic Idea of the K-means Clustering Algorithm\n",
    "The simplest of all clustering algorithms is the aptly named **k-means algorithm**\n",
    ", sometimes also referred to as *Lloyds algorithm*. It is the simplest and also\n",
    "the most common. From its simplicity it obtains both strengths and weaknesses.\n",
    "These will be discussed in more detail later. The k-means algorithm is a\n",
    "**centroid based** clustering algorithm.\n",
    "\n",
    "Assume, we are given $n$ data points and we wish to split the data into $K < n$\n",
    "different categories, or clusters. We label each cluster by an integer $k\\in\\{\n",
    "1, \\cdots, K \\}$. In the basic k-means algorithm each point is assigned to only\n",
    "one cluster $k$, and these assignments are *non-injective* i.e. many-to-one. We\n",
    "can think of these mappings as an encoder $k = C(i)$, which assigns the $i$-th\n",
    "data-point $\\bf x_i$ to the $k$-th cluster. Before we jump into the mathematics\n",
    "let us describe the k-means algorithm in words:\n",
    "1. We start with guesses / random initializations of our $k$ cluster centers / centroids\n",
    "\n",
    "2. For each centroid the points that are most similar are identified\n",
    "\n",
    "3. Then we move / replace each centroid with a coordinate average of all the points that were assigned to that centroid.\n",
    "\n",
    "4. Iterate this points 2, 3) until the centroids no longer move (to some tolerance)\n",
    "\n",
    "Now we consider the method formally. Again, we assume we have $n$ data-points\n",
    "(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a24b82d",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:kmeanspoints\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\\label{eq:kmeanspoints} \\tag{1}\n",
    "  \\boldsymbol{x_i}  = \\{x_{i, 1}, \\cdots, x_{i, p}\\}\\in\\mathbb{R}^p.\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c83fd8",
   "metadata": {
    "editable": true
   },
   "source": [
    "which we wish to group into $K < n$ clusters. For our dissimilarity measure we\n",
    "will use the *squared Euclidean distance*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd8508b",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:squaredeuclidean\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\\label{eq:squaredeuclidean} \\tag{2}\n",
    "  d(\\boldsymbol{x_i}, \\boldsymbol{x_i'}) = \\sum_{j=1}^p(x_{ij} - x_{i'j})^2\n",
    "                         = ||\\boldsymbol{x_i} - \\boldsymbol{x_{i'}}||^2\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5dcd1e",
   "metadata": {
    "editable": true
   },
   "source": [
    "Next we define the so called *within-cluster point scatter* which gives us a\n",
    "measure of how close each data point assigned to the same cluster tends to be to\n",
    "the all the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9251879c",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:withincluster\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\\label{eq:withincluster} \\tag{3}\n",
    "  W(C) = \\frac{1}{2}\\sum_{k=1}^K\\sum_{C(i)=k}\n",
    "          \\sum_{C(i')=k}d(\\boldsymbol{x_i}, \\boldsymbol{x_{i'}}) =\n",
    "          \\sum_{k=1}^KN_k\\sum_{C(i)=k}||\\boldsymbol{x_i} - \\boldsymbol{\\overline{x_k}}||^2\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f657599e",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\boldsymbol{\\overline{x_k}}$ is the mean vector associated with the $k$-th\n",
    "cluster, and $N_k = \\sum_{i=1}^nI(C(i) = k)$, where the $I()$ notation is\n",
    "similar to the Kronecker delta (*Commonly used in statistics, it just means that\n",
    "when $i = k$ we have the encoder $C(i)$*). In other words,  the within-cluster\n",
    "scatter measures the compactness of each cluster with respect to the data points\n",
    "assigned to each cluster. This is the quantity that the $k$-means algorithm aims\n",
    "to minimize. We refer to this quantity $W(C)$ as the within cluster scatter\n",
    "because of its relation to the *total scatter*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc14778",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:totalscatter\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\\label{eq:totalscatter} \\tag{4}\n",
    "  T = W(C) + B(C) = \\frac{1}{2}\\sum_{i=1}^n\n",
    "                    \\sum_{i'=1}^nd(\\boldsymbol{x_i}, \\boldsymbol{x_{i'}})\n",
    "                  = \\frac{1}{2}\\sum_{k=1}^K\\sum_{C(i)=k}\n",
    "                    \\Big(\\sum_{C(i') = k}d(\\boldsymbol{x_i}, \\boldsymbol{x_{i'}})\n",
    "                  + \\sum_{C(i')\\neq k}d(\\boldsymbol{x_i}, \\boldsymbol{x_{i'}})\\Big)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbad462",
   "metadata": {
    "editable": true
   },
   "source": [
    "Which is a quantity that is conserved throughout the $k$-means algorithm. It can\n",
    "be thought of as the total amount of information in the data, and it is composed\n",
    "of the aforementioned within-cluster scatter and the *between-cluster scatter*\n",
    "$B(C)$. In methods such as principle component analysis the total scatter is not\n",
    "conserved.\n",
    "\n",
    "Given a cluster mean $\\boldsymbol{m_k}$ we define the **total cluster variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c09a6",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:totalclustervariance\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\\label{eq:totalclustervariance} \\tag{5}\n",
    "  \\min_{C, \\{\\boldsymbol{m_k}\\}_1^K}\\sum_{k=1}^KN_k\\sum||\\boldsymbol{x_i} - \\boldsymbol{m_k}||^2\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e1b16d",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now we have all the pieces necessary to formally revisit the k-means algorithm.\n",
    "If you at this point feel like some of the above definitions came a bit out of\n",
    "no-where, don't fret, the method does get a whole lot simpler once we start\n",
    "programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860cb989",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The K-means Clustering Algorithm\n",
    "The k-means clustering algorithm goes as follows (note in my opinion this\n",
    "description is a bit complicated and is lifted directly out of ESL HASTIE for\n",
    "deeper understanding purposes)\n",
    "\n",
    "1. For a given cluster assignment $C$, and $k$ cluster means $\\{m_1, \\cdots, m_k\\}$. We minimize the total cluster variance with respect to the cluster means $\\{m_k\\}$ yielding the means of the currently assigned clusters.\n",
    "\n",
    "2. Given a current set of $k$ means $\\{m_k\\}$ the total cluster variance is minimized by assigning each observation to the closest (current) cluster mean. That is $$C(i) = \\underset{1\\leq k\\leq K}{\\mathrm{argmin}} ||\\boldsymbol{x_i} - \\boldsymbol{m_k}||^2$$\n",
    "\n",
    "3. Steps 1 and 2 are repeated until the assignments do not change.\n",
    "\n",
    "As previously stated the above formulation can be a bit difficult to understand,\n",
    "*at least the first time*, due to the dense notation used. But all in all the\n",
    "concept is fairly simple when explained in words. The math needs to be\n",
    "understood but to help you along the way we summarize the algorithm as follows\n",
    "(try to look at the terms above to match with the summary).\n",
    "\n",
    "1. Before we start we specify a number $k$ which is the number of clusters we want to try to separate our data into.\n",
    "\n",
    "2. We initially choose $k$ random data points in our data as our initial centroids, *or means* (this is where the name comes from).\n",
    "\n",
    "3. Assign each data point to their closest centroid, based on the squared Euclidean distance.\n",
    "\n",
    "4. For each of the $k$ cluster we update the centroid by calculating new mean values for all the data points in the cluster.\n",
    "\n",
    "5. Iteratively minimize the within cluster scatter by performing steps (3, 4) until the new assignments stop changing (can be to some tolerance) or until a maximum number of iterations have passed.\n",
    "\n",
    "That's it, nothing magical happening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec42c91",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Writing Our Own Code\n",
    "In the following section we will work to develop a deeper understanding of the\n",
    "previously discussed mathematics through developing codes to do k-means cluster\n",
    "analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9c1ebc",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Basic Python\n",
    "\n",
    "Let us now program the most basic version of the algorithm using nothing but\n",
    "Python with numpy arrays. This code is kept intentionally simple to gradually\n",
    "progress our understanding. There is no vectorization of any kind, and even most\n",
    "helper functions are not utilized. Throughout our implementation process it will\n",
    "be helpful to keep in mind both the mathematical description of the algorithm\n",
    "*and* our summary from above.\n",
    "\n",
    "First of all we need a dataset to do our cluster analysis on, for clarity (and\n",
    "lack of googling beforehand) we generate it ourselves using Gaussians. First we\n",
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6919b6e2",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c07378",
   "metadata": {
    "editable": true
   },
   "source": [
    "Next we define functions, for ease of use later, to generate Gaussians and to\n",
    "set up our toy data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f387e4",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gaussian_points(dim=2, n_points=1000, mean_vector=np.array([0, 0]),\n",
    "                    sample_variance=1):\n",
    "    \"\"\"\n",
    "    Very simple custom function to generate gaussian distributed point clusters\n",
    "    with variable dimension, number of points, means in each direction\n",
    "    (must match dim) and sample variance.\n",
    "\n",
    "    Inputs:\n",
    "        dim (int)\n",
    "        n_points (int)\n",
    "        mean_vector (np.array) (where index 0 is x, index 1 is y etc.)\n",
    "        sample_variance (float)\n",
    "\n",
    "    Returns:\n",
    "        data (np.array): with dimensions (dim x n_points)\n",
    "    \"\"\"\n",
    "\n",
    "    mean_matrix = np.zeros(dim) + mean_vector\n",
    "    covariance_matrix = np.eye(dim) * sample_variance\n",
    "    data = np.random.multivariate_normal(mean_matrix, covariance_matrix,\n",
    "                                    n_points)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def generate_simple_clustering_dataset(dim=2, n_points=1000, plotting=True,\n",
    "                                    return_data=True):\n",
    "    \"\"\"\n",
    "    Toy model to illustrate k-means clustering\n",
    "    \"\"\"\n",
    "\n",
    "    data1 = gaussian_points(mean_vector=np.array([5, 5]))\n",
    "    data2 = gaussian_points()\n",
    "    data3 = gaussian_points(mean_vector=np.array([1, 4.5]))\n",
    "    data4 = gaussian_points(mean_vector=np.array([5, 1]))\n",
    "    data = np.concatenate((data1, data2, data3, data4), axis=0)\n",
    "\n",
    "    if plotting:\n",
    "        fig1, ax1 = plt.subplots(figsize=(8, 5))\n",
    "        ax1.scatter(data[:, 0], data[:, 1])\n",
    "        ax1.set_aspect('equal')\n",
    "        #ax1.grid()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    if return_data:\n",
    "        return data\n",
    "\n",
    "\n",
    "data = generate_simple_clustering_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83986655",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now that we are our, albeit very simple, dataset we are ready to start\n",
    "implementing the k-means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a092d3c",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_samples, dimensions = data.shape\n",
    "k_clusters =\n",
    "\n",
    "# we randomly initialize our centroids\n",
    "centroids = data[np.random.choice(n_samples, k_clusters, replace=False), :]\n",
    "distances = np.zeros((n_samples, n_clusters))\n",
    "\n",
    "# first we need to assign each point to our initial centroids\n",
    "for k in range(n_clusters):\n",
    "    for n in range(n_samples):\n",
    "        dist = 0\n",
    "        for d in range(dimensions):\n",
    "            dist += np.abs(data[n, d] - centroids[k, d])**2\n",
    "            distances[n, k] = dist"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
